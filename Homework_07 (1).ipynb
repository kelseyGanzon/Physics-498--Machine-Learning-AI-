{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ff-98iNCXAiD",
      "metadata": {
        "id": "ff-98iNCXAiD"
      },
      "source": [
        "# Homework 07: Reinforcement Learning: Implementing a Deep Q-Network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e98e218",
      "metadata": {
        "id": "0e98e218"
      },
      "source": [
        "Make sure to run every single cell in this notebook, or some libraries might be missing. Also, if you are using Colab, make sure to **change your Runtime (change runtime type under Runtime)** to a GPU, although it will work on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe33ace4",
      "metadata": {
        "id": "fe33ace4",
        "outputId": "968d5f19-c941-4db6-f219-dceeaee011f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a-utiRozzioB",
      "metadata": {
        "id": "a-utiRozzioB"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import pdb\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PaTer9KwXAiG",
      "metadata": {
        "id": "PaTer9KwXAiG"
      },
      "source": [
        "## <span style=\"color:LightGreen\">Implementing a DQN</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a306bfd0",
      "metadata": {
        "id": "a306bfd0"
      },
      "source": [
        "In this assignment you will be implementing a Deep Q-network with replay memory to play the Cartpole environment. Recall that for a DQN, we are approximating the Q-value table in Q-learning with a neural network.\n",
        "\n",
        "The general design is a DQN neural network class that you will implemented with a DQNAgent wrapper on top. The DQNAgent controls all the weight updates and environment interactions by calling the DQN when necessary. You will also be filling in the exploration policy and you will be provided the helper class for replay memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "397c2f51",
      "metadata": {
        "id": "397c2f51"
      },
      "source": [
        "## <span style=\"color:Orange\">Problem 1</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "38e284a7",
      "metadata": {
        "id": "38e284a7"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    #grabbed this from our RL lecture, modified to take number of hidden layers as a variable\n",
        "    def __init__(self, n_observations, n_hidden, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_observations, n_hidden)\n",
        "        self.layer2 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.layer3 = nn.Linear(n_hidden, n_actions)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6933fb7b",
      "metadata": {
        "id": "6933fb7b"
      },
      "outputs": [],
      "source": [
        "\n",
        "class QNetwork():\n",
        "    # This class essentially defines the network architecture.\n",
        "    # It is NOT the PyTorch Q-network model (nn.Module), but a wrapper\n",
        "    # The network should take in state of the world as an input,\n",
        "    # and output Q values of the actions available to the agent as the output.\n",
        "\n",
        "    def __init__(self, args, input, output, learning_rate):\n",
        "        # Define your network architecture here. It is also a good idea to define any training operations\n",
        "        # and optimizers here, initialize your variables, or alternately compile your model here.\n",
        "        self.weights_path = 'models/%s/%s' % (args['env'], datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
        "\n",
        "        # Network architecture.\n",
        "        self.hidden = 128\n",
        "        self.model = DQN(input, self.hidden, output)\n",
        "\n",
        "        # Loss and optimizer.\n",
        "        self.optim = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        if args['model_file'] is not None:\n",
        "            print('Loading pretrained model from', args['model_file'])\n",
        "            self.load_model_weights(args['model_file'])\n",
        "\n",
        "    def save_model_weights(self, step):\n",
        "        # Helper function to save your model / weights.\n",
        "        if not os.path.exists(self.weights_path): os.makedirs(self.weights_path)\n",
        "        torch.save(self.model.state_dict(), os.path.join(self.weights_path, 'model_%d.h5' % step))\n",
        "\n",
        "    def load_model_weights(self, weight_file):\n",
        "        # Helper function to load model weights.\n",
        "        self.model.load_state_dict(torch.load(weight_file))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4995b097",
      "metadata": {
        "id": "4995b097"
      },
      "source": [
        "### <span style=\"color:Yellow\">Replay Memory Helper</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50ae7da8",
      "metadata": {
        "id": "50ae7da8"
      },
      "source": [
        "Replay memory or Experience replay is a simple trick used to learn the Q-value network offline. It also ensures the model can learn from past experiences without weighting heavily towards current observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "885bb32f",
      "metadata": {
        "id": "885bb32f"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Replay_Memory():\n",
        "    def __init__(self, state_dim, action_dim, memory_size=50000, burn_in=10000):\n",
        "        # The memory essentially stores transitions recorder from the agent\n",
        "        # taking actions in the environment.\n",
        "\n",
        "        # Burn in episodes define the number of episodes that are written into the memory from the\n",
        "        # randomly initialized agent. Memory size is the maximum size after which old elements in the memory are replaced.\n",
        "        # A simple (if not the most efficient) way to implement the memory is as a list of transitions.\n",
        "        self.memory_size = memory_size\n",
        "        self.burn_in = burn_in\n",
        "        self.states = torch.zeros((self.memory_size, state_dim))\n",
        "        self.next_states = torch.zeros((self.memory_size, state_dim))\n",
        "        self.actions = torch.zeros((self.memory_size, 1))\n",
        "        self.rewards = torch.zeros((self.memory_size, 1))\n",
        "        self.dones = torch.zeros((self.memory_size, 1))\n",
        "        self.ptr = 0\n",
        "        self.burned_in = False\n",
        "        self.not_full_yet = True\n",
        "\n",
        "    def append(self, states, actions, rewards, next_states, dones):\n",
        "        self.states[self.ptr] = states\n",
        "        self.actions[self.ptr, 0] = actions\n",
        "        self.rewards[self.ptr, 0] = rewards\n",
        "        self.next_states[self.ptr] = next_states\n",
        "        self.dones[self.ptr, 0] = dones\n",
        "        self.ptr += 1\n",
        "\n",
        "        if self.ptr > self.burn_in:\n",
        "            self.burned_in = True\n",
        "\n",
        "        if self.ptr >= self.memory_size:\n",
        "            self.ptr = 0\n",
        "            self.not_full_yet = False\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        # This function returns a batch of randomly sampled transitions - i.e. state, action, reward, next state, terminal flag tuples.\n",
        "        # You will feed this to your model to train.\n",
        "        if self.not_full_yet:\n",
        "            idxs = torch.from_numpy(np.random.choice(self.ptr, batch_size, False))\n",
        "        else:\n",
        "            idxs = torch.from_numpy(np.random.choice(self.memory_size, batch_size, False))\n",
        "\n",
        "        states = self.states[idxs]\n",
        "        next_states = self.next_states[idxs]\n",
        "        actions = self.actions[idxs]\n",
        "        rewards = self.rewards[idxs]\n",
        "        dones = self.dones[idxs]\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d65834ca",
      "metadata": {
        "id": "d65834ca"
      },
      "source": [
        "## <span style=\"color:LightGreen\">The Agent Class</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5d5e257",
      "metadata": {
        "id": "f5d5e257"
      },
      "source": [
        "This section is focused on building the agent that interacts with the environment. The agent wrapper defines a policy (which you will implement), as well as all of the functionality for learning the network and using experience replay. You will implement a large chunk of this class."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50675691",
      "metadata": {
        "id": "50675691"
      },
      "source": [
        "## <span style=\"color:Orange\">Problem 2</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e6f37788",
      "metadata": {
        "id": "e6f37788"
      },
      "outputs": [],
      "source": [
        "class DQN_Agent():\n",
        "    def __init__(self, args):\n",
        "        # Create an instance of the network itself, as well as the memory.\n",
        "        # Here is also a good place to set environmental parameters,\n",
        "        # as well as training parameters - number of episodes / iterations, etc.\n",
        "\n",
        "        # Inputs\n",
        "        self.args = args\n",
        "        self.environment_name = self.args['env']\n",
        "        self.render = self.args['render']\n",
        "        self.epsilon = args['epsilon']\n",
        "        self.network_update_freq = args['network_update_freq']\n",
        "        self.log_freq = args['log_freq']\n",
        "        self.test_freq = args['test_freq']\n",
        "        self.save_freq = args['save_freq']\n",
        "        self.learning_rate = args['learning_rate']\n",
        "\n",
        "        # Env related variables\n",
        "        if self.environment_name == 'CartPole-v0':\n",
        "            self.env = gym.make(self.environment_name, render_mode='rgb_array')\n",
        "            self.discount_factor = 0.99\n",
        "            self.num_episodes = 5000\n",
        "        elif self.environment_name == 'MountainCar-v0':\n",
        "            self.env = gym.make(self.environment_name, render_mode='rgb_array')\n",
        "            self.discount_factor = 1.00\n",
        "            self.num_episodes = 10000\n",
        "        else:\n",
        "            raise Exception(\"Unknown Environment\")\n",
        "\n",
        "        # Other Classes\n",
        "        self.q_network = QNetwork(args, self.env.observation_space.shape[0], self.env.action_space.n, self.learning_rate)\n",
        "        self.target_q_network = QNetwork(args, self.env.observation_space.shape[0], self.env.action_space.n, self.learning_rate)\n",
        "        self.memory = Replay_Memory(self.env.observation_space.shape[0], self.env.action_space.n, memory_size=args['memory_size'])\n",
        "\n",
        "        # Plotting\n",
        "        self.rewards = []\n",
        "        self.td_error = []\n",
        "        self.batch = list(range(32))\n",
        "\n",
        "        # Save hyperparameters\n",
        "        self.logdir = 'logs/%s/%s' % (self.environment_name, datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
        "        if not os.path.exists(self.logdir):\n",
        "            os.makedirs(self.logdir)\n",
        "        with open(self.logdir + '/hyperparameters.json', 'w') as outfile:\n",
        "            json.dump((self.args), outfile, indent=4)\n",
        "\n",
        "    def epsilon_greedy_policy(self, q_values, epsilon):\n",
        "        # Creating epsilon greedy probabilities to sample from.\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "\n",
        "        else:\n",
        "            action = torch.argmax(q_values)\n",
        "        return int(action)\n",
        "\n",
        "\n",
        "    def greedy_policy(self, q_values):\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "\n",
        "        action = torch.argmax(q_values)\n",
        "        return int(action)\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self.burn_in_memory()\n",
        "        for step in range(self.num_episodes):\n",
        "            # Generate Episodes using Epsilon Greedy Policy and train the Q network.\n",
        "            self.generate_episode(policy=self.epsilon_greedy_policy, mode='train',\n",
        "                epsilon=self.epsilon, frameskip=self.args['frameskip'])\n",
        "\n",
        "            # Test the network.\n",
        "            if step % self.test_freq == 0:\n",
        "                test_reward, test_error = self.test(episodes=20)\n",
        "                self.rewards.append([test_reward, step])\n",
        "                self.td_error.append([test_error, step])\n",
        "\n",
        "            # Update the target network.\n",
        "            if step % self.network_update_freq == 0:\n",
        "                self.hard_update()\n",
        "\n",
        "            # Logging.\n",
        "            if step % self.log_freq == 0:\n",
        "                print(\"Step: {0:05d}/{1:05d}\".format(step, self.num_episodes))\n",
        "\n",
        "            # Save the model.\n",
        "            if step % self.save_freq == 0:\n",
        "                self.q_network.save_model_weights(step)\n",
        "\n",
        "            step += 1\n",
        "            self.epsilon_decay()\n",
        "\n",
        "            # Render and save the video with the model.\n",
        "            if step % int(self.num_episodes / 3) == 0 and self.args['render']:\n",
        "                # test_video(self, self.environment_name, step)\n",
        "                self.q_network.save_model_weights(step)\n",
        "\n",
        "\n",
        "    def td_estimate (self, state, action):\n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "        q_values = self.q_network.model(state).gather(1, action.type(torch.int64))\n",
        "        return q_values\n",
        "\n",
        "    def td_target (self, reward, next_state, done):\n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "\n",
        "        next_q_values = self.target_q_network.model(next_state).detach().max(1).values\n",
        "        target = reward + self.discount_factor * (1-done) * torch.max(next_q_values)\n",
        "        return target\n",
        "\n",
        "    def train_dqn(self):\n",
        "        # Sample from the replay buffer.\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample_batch(batch_size=32)\n",
        "\n",
        "\n",
        "        #YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        estimates = self.td_estimate(states, actions)\n",
        "        target = self.td_target(rewards, next_states, dones)\n",
        "        loss = F.smooth_l1_loss(estimates, target)\n",
        "\n",
        "        self.q_network.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.q_network.optim.step()\n",
        "\n",
        "\n",
        "    def hard_update(self):\n",
        "        self.target_q_network.model.load_state_dict(self.q_network.model.state_dict())\n",
        "\n",
        "    def test(self, model_file=None, episodes=100):\n",
        "\n",
        "        cum_reward = []\n",
        "        td_error = []\n",
        "        for count in range(episodes):\n",
        "            reward, error = self.generate_episode(policy=self.epsilon_greedy_policy,\n",
        "                mode='test', epsilon=0.05, frameskip=self.args['frameskip'])\n",
        "            cum_reward.append(reward)\n",
        "            td_error.append(error)\n",
        "        cum_reward = torch.tensor(cum_reward)\n",
        "        td_error = torch.tensor(td_error)\n",
        "        print(\"\\nTest Rewards: {0} | TD Error: {1:.4f}\\n\".format(torch.mean(cum_reward), torch.mean(td_error)))\n",
        "        return torch.mean(cum_reward), torch.mean(td_error)\n",
        "\n",
        "    def burn_in_memory(self):\n",
        "        # Initialize your replay memory with a burn_in number of episodes / transitions.\n",
        "        while not self.memory.burned_in:\n",
        "            self.generate_episode(policy=self.epsilon_greedy_policy, mode='burn_in',\n",
        "                epsilon=self.epsilon, frameskip=self.args['frameskip'])\n",
        "        print(\"Burn Complete!\")\n",
        "\n",
        "    def generate_episode(self, policy, epsilon, mode='train', frameskip=1):\n",
        "        \"\"\"\n",
        "        Collects one rollout from the policy in an environment.\n",
        "        \"\"\"\n",
        "        done = False\n",
        "        state = torch.from_numpy(self.env.reset())\n",
        "        rewards = 0\n",
        "        q_values = self.q_network.model.forward((state.reshape(1, -1)))\n",
        "        td_error = []\n",
        "        while not done:\n",
        "            action = policy(q_values, epsilon)\n",
        "            i = 0\n",
        "            while (i < frameskip) and not done:\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = torch.from_numpy(next_state)\n",
        "                rewards += reward\n",
        "                i += 1\n",
        "            next_q_values = self.q_network.model.forward((next_state.reshape(1, -1)))\n",
        "            if mode in ['train', 'burn_in'] :\n",
        "                self.memory.append(state, action, reward, next_state, done)\n",
        "            else:\n",
        "                td_error.append(abs(reward + self.discount_factor * (1 - done) * torch.max(next_q_values) - q_values))\n",
        "            if not done:\n",
        "                state = copy.deepcopy(next_state.detach())\n",
        "                q_values = copy.deepcopy(next_q_values.detach())\n",
        "\n",
        "            # Train the network.\n",
        "            if mode == 'train':\n",
        "                self.train_dqn()\n",
        "        if td_error == []:\n",
        "          return rewards, []\n",
        "        return rewards, torch.mean(torch.stack(td_error))\n",
        "\n",
        "    def plots(self):\n",
        "        \"\"\"\n",
        "        Plots:\n",
        "        1) Avg Cummulative Test Reward over 20 Plots\n",
        "        2) TD Error\n",
        "        \"\"\"\n",
        "        reward, time =  zip(*self.rewards)\n",
        "        plt.figure(figsize=(8, 3))\n",
        "        plt.subplot(121)\n",
        "        plt.title('Cummulative Reward')\n",
        "        plt.plot(time, reward)\n",
        "        plt.xlabel('iterations')\n",
        "        plt.ylabel('rewards')\n",
        "        plt.legend()\n",
        "        plt.ylim([0, None])\n",
        "\n",
        "        loss, time =  zip(*self.td_error)\n",
        "        plt.subplot(122)\n",
        "        plt.title('Loss')\n",
        "        plt.plot(time, loss)\n",
        "        plt.xlabel('iterations')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "\n",
        "    def epsilon_decay(self, initial_eps=1.0, final_eps=0.05):\n",
        "        if(self.epsilon > final_eps):\n",
        "            factor = (initial_eps - final_eps) / 10000\n",
        "            self.epsilon -= factor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fef9f0ba",
      "metadata": {
        "id": "fef9f0ba"
      },
      "source": [
        "### <span style=\"color:Yellow\">Helpers and Hyperparameters</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b99ee449",
      "metadata": {
        "id": "b99ee449"
      },
      "source": [
        "This class contains helper functions, as well as some extra arguments that you can use to tune or play around with your DQN. There is no required parts to fill in here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "efb74749",
      "metadata": {
        "id": "efb74749"
      },
      "outputs": [],
      "source": [
        "# Note: if you have problems creating video captures on servers without GUI,\n",
        "#       you could save and relaod model to create videos on your laptop.\n",
        "def test_video(agent, env_name, episodes):\n",
        "    # Usage:\n",
        "    #   you can pass the arguments within agent.train() as:\n",
        "    #       if episode % int(self.num_episodes/3) == 0:\n",
        "    #           test_video(self, self.environment_name, episode)\n",
        "    save_path = \"%s/video-%s\" % (env_name, episodes)\n",
        "    if not os.path.exists(save_path): os.makedirs(save_path)\n",
        "\n",
        "    # To create video\n",
        "    env = agent.env # gym.wrappers.Monitor(agent.env, save_path, force=True)\n",
        "    reward_total = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    print(\"Video recording the agent with epsilon {0:.4f}\".format(agent.epsilon))\n",
        "    while not done:\n",
        "        q_values = agent.q_network.model.forward(torch.from_numpy(state.reshape(1, -1)))\n",
        "        action = agent.greedy_policy(q_values)\n",
        "        i = 0\n",
        "        while (i < agent.args['frameskip']) and not done:\n",
        "            screen = env.render(mode='rgb_array')\n",
        "            plt.imshow(screen[0])\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            reward_total.append(reward)\n",
        "            i += 1\n",
        "        state = next_state\n",
        "    print(\"reward_total: {}\".format(torch.sum(torch.tensor(reward_total))))\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    env.close()\n",
        "\n",
        "\n",
        "def init_flags():\n",
        "\n",
        "    flags = {\n",
        "        \"env\": \"CartPole-v0\", # Change to \"MountainCar-v0\" when needed.\n",
        "        \"render\": False,\n",
        "        \"train\": 1,\n",
        "        \"frameskip\" : 1,\n",
        "        \"network_update_freq\": 10,\n",
        "        \"log_freq\": 25,\n",
        "        \"test_freq\": 100,\n",
        "        \"save_freq\": 500,\n",
        "        \"learning_rate\": 5e-4,\n",
        "        \"memory_size\": 50000,\n",
        "        \"epsilon\": 0.5,\n",
        "        \"model_file\": None,\n",
        "    }\n",
        "\n",
        "    return flags\n",
        "\n",
        "\n",
        "def main(render=False):\n",
        "    args = init_flags()\n",
        "    args[\"render\"] = render\n",
        "\n",
        "    # You want to create an instance of the DQN_Agent class here, and then train / test it.\n",
        "    q_agent = DQN_Agent(args)\n",
        "\n",
        "    # Render output videos using the model loaded from file.\n",
        "    if args['render']: test_video(q_agent, args['env'], 1)\n",
        "    else: q_agent.train()  # Train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0a5c5b3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a5c5b3e",
        "outputId": "a305ad90-a271-49f7-9e94-38edd127aa5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Burn Complete!\n",
            "\n",
            "Test Rewards: 9.75 | TD Error: 1.1344\n",
            "\n",
            "Step: 00000/05000\n",
            "Step: 00025/05000\n",
            "Step: 00050/05000\n",
            "Step: 00075/05000\n",
            "\n",
            "Test Rewards: 15.5 | TD Error: 1.4911\n",
            "\n",
            "Step: 00100/05000\n",
            "Step: 00125/05000\n",
            "Step: 00150/05000\n",
            "Step: 00175/05000\n",
            "\n",
            "Test Rewards: 16.100000381469727 | TD Error: 1.9711\n",
            "\n",
            "Step: 00200/05000\n",
            "Step: 00225/05000\n",
            "Step: 00250/05000\n",
            "Step: 00275/05000\n",
            "\n",
            "Test Rewards: 9.350000381469727 | TD Error: 3.5493\n",
            "\n",
            "Step: 00300/05000\n",
            "Step: 00325/05000\n",
            "Step: 00350/05000\n",
            "Step: 00375/05000\n",
            "\n",
            "Test Rewards: 9.75 | TD Error: 4.2696\n",
            "\n",
            "Step: 00400/05000\n",
            "Step: 00425/05000\n",
            "Step: 00450/05000\n",
            "Step: 00475/05000\n",
            "\n",
            "Test Rewards: 169.75 | TD Error: 0.8458\n",
            "\n",
            "Step: 00500/05000\n",
            "Step: 00525/05000\n",
            "Step: 00550/05000\n",
            "Step: 00575/05000\n",
            "\n",
            "Test Rewards: 13.800000190734863 | TD Error: 4.4912\n",
            "\n",
            "Step: 00600/05000\n",
            "Step: 00625/05000\n",
            "Step: 00650/05000\n",
            "Step: 00675/05000\n",
            "\n",
            "Test Rewards: 10.0 | TD Error: 6.6826\n",
            "\n",
            "Step: 00700/05000\n",
            "Step: 00725/05000\n",
            "Step: 00750/05000\n",
            "Step: 00775/05000\n",
            "\n",
            "Test Rewards: 10.0 | TD Error: 7.6117\n",
            "\n",
            "Step: 00800/05000\n",
            "Step: 00825/05000\n",
            "Step: 00850/05000\n",
            "Step: 00875/05000\n",
            "\n",
            "Test Rewards: 11.300000190734863 | TD Error: 7.2524\n",
            "\n",
            "Step: 00900/05000\n",
            "Step: 00925/05000\n",
            "Step: 00950/05000\n",
            "Step: 00975/05000\n",
            "\n",
            "Test Rewards: 9.75 | TD Error: 8.9150\n",
            "\n",
            "Step: 01000/05000\n",
            "Step: 01025/05000\n",
            "Step: 01050/05000\n",
            "Step: 01075/05000\n",
            "\n",
            "Test Rewards: 9.850000381469727 | TD Error: 9.5397\n",
            "\n",
            "Step: 01100/05000\n",
            "Step: 01125/05000\n",
            "Step: 01150/05000\n",
            "Step: 01175/05000\n",
            "\n",
            "Test Rewards: 10.149999618530273 | TD Error: 10.1589\n",
            "\n",
            "Step: 01200/05000\n",
            "Step: 01225/05000\n",
            "Step: 01250/05000\n",
            "Step: 01275/05000\n",
            "\n",
            "Test Rewards: 11.600000381469727 | TD Error: 9.4878\n",
            "\n",
            "Step: 01300/05000\n",
            "Step: 01325/05000\n",
            "Step: 01350/05000\n",
            "Step: 01375/05000\n",
            "\n",
            "Test Rewards: 9.550000190734863 | TD Error: 12.6757\n",
            "\n",
            "Step: 01400/05000\n",
            "Step: 01425/05000\n",
            "Step: 01450/05000\n",
            "Step: 01475/05000\n",
            "\n",
            "Test Rewards: 9.649999618530273 | TD Error: 15.6036\n",
            "\n",
            "Step: 01500/05000\n",
            "Step: 01525/05000\n",
            "Step: 01550/05000\n",
            "Step: 01575/05000\n",
            "\n",
            "Test Rewards: 9.550000190734863 | TD Error: 17.7541\n",
            "\n",
            "Step: 01600/05000\n",
            "Step: 01625/05000\n",
            "Step: 01650/05000\n",
            "Step: 01675/05000\n",
            "\n",
            "Test Rewards: 9.800000190734863 | TD Error: 16.8489\n",
            "\n",
            "Step: 01700/05000\n",
            "Step: 01725/05000\n",
            "Step: 01750/05000\n",
            "Step: 01775/05000\n",
            "\n",
            "Test Rewards: 9.699999809265137 | TD Error: 18.1988\n",
            "\n",
            "Step: 01800/05000\n",
            "Step: 01825/05000\n",
            "Step: 01850/05000\n",
            "Step: 01875/05000\n",
            "\n",
            "Test Rewards: 10.050000190734863 | TD Error: 17.4072\n",
            "\n",
            "Step: 01900/05000\n",
            "Step: 01925/05000\n",
            "Step: 01950/05000\n",
            "Step: 01975/05000\n",
            "\n",
            "Test Rewards: 9.899999618530273 | TD Error: 18.1716\n",
            "\n",
            "Step: 02000/05000\n",
            "Step: 02025/05000\n",
            "Step: 02050/05000\n",
            "Step: 02075/05000\n",
            "\n",
            "Test Rewards: 9.550000190734863 | TD Error: 20.9368\n",
            "\n",
            "Step: 02100/05000\n",
            "Step: 02125/05000\n",
            "Step: 02150/05000\n",
            "Step: 02175/05000\n",
            "\n",
            "Test Rewards: 9.300000190734863 | TD Error: 17.7665\n",
            "\n",
            "Step: 02200/05000\n",
            "Step: 02225/05000\n",
            "Step: 02250/05000\n",
            "Step: 02275/05000\n",
            "\n",
            "Test Rewards: 9.800000190734863 | TD Error: 18.7955\n",
            "\n",
            "Step: 02300/05000\n",
            "Step: 02325/05000\n",
            "Step: 02350/05000\n",
            "Step: 02375/05000\n",
            "\n",
            "Test Rewards: 9.25 | TD Error: 23.2191\n",
            "\n",
            "Step: 02400/05000\n",
            "Step: 02425/05000\n",
            "Step: 02450/05000\n",
            "Step: 02475/05000\n",
            "\n",
            "Test Rewards: 9.399999618530273 | TD Error: 21.9684\n",
            "\n",
            "Step: 02500/05000\n",
            "Step: 02525/05000\n",
            "Step: 02550/05000\n",
            "Step: 02575/05000\n",
            "\n",
            "Test Rewards: 9.75 | TD Error: 17.6447\n",
            "\n",
            "Step: 02600/05000\n",
            "Step: 02625/05000\n",
            "Step: 02650/05000\n",
            "Step: 02675/05000\n",
            "\n",
            "Test Rewards: 9.649999618530273 | TD Error: 20.9428\n",
            "\n",
            "Step: 02700/05000\n",
            "Step: 02725/05000\n",
            "Step: 02750/05000\n",
            "Step: 02775/05000\n",
            "\n",
            "Test Rewards: 10.149999618530273 | TD Error: 22.1825\n",
            "\n",
            "Step: 02800/05000\n",
            "Step: 02825/05000\n",
            "Step: 02850/05000\n",
            "Step: 02875/05000\n",
            "\n",
            "Test Rewards: 13.350000381469727 | TD Error: 14.1098\n",
            "\n",
            "Step: 02900/05000\n",
            "Step: 02925/05000\n",
            "Step: 02950/05000\n",
            "Step: 02975/05000\n",
            "\n",
            "Test Rewards: 10.949999809265137 | TD Error: 19.2671\n",
            "\n",
            "Step: 03000/05000\n",
            "Step: 03025/05000\n",
            "Step: 03050/05000\n",
            "Step: 03075/05000\n",
            "\n",
            "Test Rewards: 9.75 | TD Error: 19.9135\n",
            "\n",
            "Step: 03100/05000\n",
            "Step: 03125/05000\n",
            "Step: 03150/05000\n",
            "Step: 03175/05000\n",
            "\n",
            "Test Rewards: 9.449999809265137 | TD Error: 25.3757\n",
            "\n",
            "Step: 03200/05000\n",
            "Step: 03225/05000\n",
            "Step: 03250/05000\n",
            "Step: 03275/05000\n",
            "\n",
            "Test Rewards: 14.199999809265137 | TD Error: 14.7892\n",
            "\n",
            "Step: 03300/05000\n",
            "Step: 03325/05000\n",
            "Step: 03350/05000\n",
            "Step: 03375/05000\n",
            "\n",
            "Test Rewards: 13.649999618530273 | TD Error: 14.6524\n",
            "\n",
            "Step: 03400/05000\n",
            "Step: 03425/05000\n",
            "Step: 03450/05000\n",
            "Step: 03475/05000\n",
            "\n",
            "Test Rewards: 11.600000381469727 | TD Error: 18.2633\n",
            "\n",
            "Step: 03500/05000\n",
            "Step: 03525/05000\n",
            "Step: 03550/05000\n",
            "Step: 03575/05000\n",
            "\n",
            "Test Rewards: 10.449999809265137 | TD Error: 19.9248\n",
            "\n",
            "Step: 03600/05000\n",
            "Step: 03625/05000\n",
            "Step: 03650/05000\n",
            "Step: 03675/05000\n",
            "\n",
            "Test Rewards: 9.449999809265137 | TD Error: 22.1462\n",
            "\n",
            "Step: 03700/05000\n",
            "Step: 03725/05000\n",
            "Step: 03750/05000\n",
            "Step: 03775/05000\n",
            "\n",
            "Test Rewards: 10.899999618530273 | TD Error: 18.3048\n",
            "\n",
            "Step: 03800/05000\n",
            "Step: 03825/05000\n",
            "Step: 03850/05000\n",
            "Step: 03875/05000\n",
            "\n",
            "Test Rewards: 9.699999809265137 | TD Error: 20.0328\n",
            "\n",
            "Step: 03900/05000\n",
            "Step: 03925/05000\n",
            "Step: 03950/05000\n",
            "Step: 03975/05000\n",
            "\n",
            "Test Rewards: 9.600000381469727 | TD Error: 18.9303\n",
            "\n",
            "Step: 04000/05000\n",
            "Step: 04025/05000\n",
            "Step: 04050/05000\n",
            "Step: 04075/05000\n",
            "\n",
            "Test Rewards: 9.850000381469727 | TD Error: 21.8418\n",
            "\n",
            "Step: 04100/05000\n",
            "Step: 04125/05000\n",
            "Step: 04150/05000\n",
            "Step: 04175/05000\n",
            "\n",
            "Test Rewards: 10.449999809265137 | TD Error: 18.5323\n",
            "\n",
            "Step: 04200/05000\n",
            "Step: 04225/05000\n",
            "Step: 04250/05000\n",
            "Step: 04275/05000\n",
            "\n",
            "Test Rewards: 13.600000381469727 | TD Error: 13.0756\n",
            "\n",
            "Step: 04300/05000\n",
            "Step: 04325/05000\n",
            "Step: 04350/05000\n",
            "Step: 04375/05000\n",
            "\n",
            "Test Rewards: 11.199999809265137 | TD Error: 15.5803\n",
            "\n",
            "Step: 04400/05000\n",
            "Step: 04425/05000\n",
            "Step: 04450/05000\n",
            "Step: 04475/05000\n",
            "\n",
            "Test Rewards: 9.399999618530273 | TD Error: 21.3797\n",
            "\n",
            "Step: 04500/05000\n",
            "Step: 04525/05000\n",
            "Step: 04550/05000\n",
            "Step: 04575/05000\n",
            "\n",
            "Test Rewards: 9.550000190734863 | TD Error: 18.0074\n",
            "\n",
            "Step: 04600/05000\n",
            "Step: 04625/05000\n",
            "Step: 04650/05000\n",
            "Step: 04675/05000\n",
            "\n",
            "Test Rewards: 9.600000381469727 | TD Error: 20.0198\n",
            "\n",
            "Step: 04700/05000\n",
            "Step: 04725/05000\n",
            "Step: 04750/05000\n",
            "Step: 04775/05000\n",
            "\n",
            "Test Rewards: 9.75 | TD Error: 18.1712\n",
            "\n",
            "Step: 04800/05000\n",
            "Step: 04825/05000\n",
            "Step: 04850/05000\n",
            "Step: 04875/05000\n",
            "\n",
            "Test Rewards: 9.199999809265137 | TD Error: 19.4433\n",
            "\n",
            "Step: 04900/05000\n",
            "Step: 04925/05000\n",
            "Step: 04950/05000\n",
            "Step: 04975/05000\n"
          ]
        }
      ],
      "source": [
        "# For training\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bfd6c8e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "bfd6c8e3",
        "outputId": "4c20241b-e7bc-4964-d5e7-3bad9339cefc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# For evaluating video.\n",
        "main(render=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PNjW2opHlVa0",
      "metadata": {
        "id": "PNjW2opHlVa0"
      },
      "source": [
        "The reward output got deleted for some reason, but I got a reward of 150, and the \"video\" ran for 9 minutes 40 seconds. I'm calling that successfully trained."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wGJg7hk3zuws",
      "metadata": {
        "id": "wGJg7hk3zuws"
      },
      "source": [
        "## <span style=\"color:Orange\">Appendix</span>\n",
        "\n",
        "### <span style=\"color:LightGreen\">Resources</span>\n",
        "\n",
        "There are many online resources for Reinforcement Learning and DQNs. Please search for them and use them as helpful background, with proper citations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "512ba1cf",
      "metadata": {
        "id": "512ba1cf"
      },
      "source": [
        "## <span style=\"color:Orange\">Acknowledgments</span>\n",
        "\n",
        "* Initial version: Mark Neubauer\n",
        "\n",
        " Copyright 2024"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}